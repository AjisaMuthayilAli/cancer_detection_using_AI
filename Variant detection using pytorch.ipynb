{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33997a8f",
   "metadata": {},
   "source": [
    "# pytorch training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4446c89c",
   "metadata": {},
   "source": [
    "def one_hot_encode(seq):\n",
    "    \"\"\"\n",
    "    Given a DNA sequence, return its one-hot encoding\n",
    "    \"\"\"\n",
    "    # Make sure seq has only allowed bases\n",
    "    allowed = set(\"ACTGN\")\n",
    "    if not set(seq).issubset(allowed):\n",
    "        invalid = set(seq) - allowed\n",
    "        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b7db9ce",
   "metadata": {},
   "source": [
    "# Create array from nucleotide sequence\n",
    "vec=np.array([nuc_d[x] for x in seq])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726ee47",
   "metadata": {},
   "source": [
    "# train_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4b47f1c",
   "metadata": {},
   "source": [
    "def quick_split(df, split_frac=0.8):\n",
    "    '''\n",
    "    Given a df of samples, randomly split indices between\n",
    "    train and test at the desired fraction\n",
    "    '''\n",
    "    cols = df.columns # original columns, use to clean up reindexed cols\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # shuffle indices\n",
    "    idxs = list(range(df.shape[0]))\n",
    "    random.shuffle(idxs)\n",
    "\n",
    "    # split shuffled index list by split_frac\n",
    "    split = int(len(idxs)*split_frac)\n",
    "    train_idxs = idxs[:split]\n",
    "    test_idxs = idxs[split:]\n",
    "    \n",
    "    # split dfs and return\n",
    "    train_df = df[df.index.isin(train_idxs)]\n",
    "    test_df = df[df.index.isin(test_idxs)]\n",
    "        \n",
    "    return train_df[cols], test_df[cols]\n",
    "    \n",
    "    \n",
    "full_train_df, test_df = quick_split(mer8)\n",
    "train_df, val_df = quick_split(full_train_df)\n",
    "\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Val:\", val_df.shape)\n",
    "print(\"Test:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33856598",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf783a82",
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "## Here is a custom defined Dataset object specialized for one-hot encoded DNA:\n",
    "\n",
    "class SeqDatasetOHE(Dataset):\n",
    "    '''\n",
    "    Dataset for one-hot-encoded sequences\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 seq_col='seq',\n",
    "                 target_col='score'\n",
    "                ):\n",
    "        # +--------------------+\n",
    "        # | Get the X examples |\n",
    "        # +--------------------+\n",
    "        # extract the DNA from the appropriate column in the df\n",
    "        self.seqs = list(df[seq_col].values)\n",
    "        self.seq_len = len(self.seqs[0])\n",
    "        \n",
    "        # one-hot encode sequences, then stack in a torch tensor\n",
    "        self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x)) for x in self.seqs])\n",
    "    \n",
    "        # +------------------+\n",
    "        # | Get the Y labels |\n",
    "        # +------------------+\n",
    "        self.labels = torch.tensor(list(df[target_col].values)).unsqueeze(1)\n",
    "        \n",
    "    def __len__(self): return len(self.seqs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # Given an index, return a tuple of an X with it's associated Y\n",
    "        # This is called inside DataLoader\n",
    "        seq = self.ohe_seqs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return seq, label\n",
    "\n",
    "\n",
    "## Here is how I constructed DataLoaders from Datasets.\n",
    "\n",
    "def build_dataloaders(train_df,\n",
    "                      test_df,\n",
    "                      seq_col='seq',\n",
    "                      target_col='score',\n",
    "                      batch_size=128,\n",
    "                      shuffle=True\n",
    "                     ):\n",
    "    '''\n",
    "    Given a train and test df with some batch construction\n",
    "    details, put them into custom SeqDatasetOHE() objects. \n",
    "    Give the Datasets to the DataLoaders and return.\n",
    "    '''\n",
    "    \n",
    "    # create Datasets    \n",
    "    train_ds = SeqDatasetOHE(train_df,seq_col=seq_col,target_col=target_col)\n",
    "    test_ds = SeqDatasetOHE(test_df,seq_col=seq_col,target_col=target_col)\n",
    "\n",
    "    # Put DataSets into DataLoaders\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_dl = DataLoader(test_ds, batch_size=batch_size)\n",
    "    \n",
    "    return train_dl,test_dl\n",
    "    \n",
    "    \n",
    "train_dl, val_dl = build_dataloaders(train_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201a0df",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6299d6f",
   "metadata": {},
   "source": [
    "# very simple linear model\n",
    "class DNA_Linear(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        # the 4 is for our one-hot encoded vector length 4!\n",
    "        self.lin = nn.Linear(4*seq_len, 1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # reshape to flatten sequence dimension\n",
    "        xb = xb.view(xb.shape[0],self.seq_len*4)\n",
    "        # Linear wraps up the weights/bias dot product operations\n",
    "        out = self.lin(xb)\n",
    "        return out\n",
    "\n",
    "# basic CNN model\n",
    "class DNA_CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 seq_len,\n",
    "                 num_filters=32,\n",
    "                 kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.conv_net = nn.Sequential(\n",
    "            # 4 is for the 4 nucleotides\n",
    "            nn.Conv1d(4, num_filters, kernel_size=kernel_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_filters*(seq_len-kernel_size+1), 1)\n",
    "        ) \n",
    "\n",
    "    def forward(self, xb):\n",
    "        # permute to put channel in correct order\n",
    "        # (batch_size x 4channel x seq_len)\n",
    "        xb = xb.permute(0,2,1) \n",
    "        \n",
    "        #print(xb.shape)\n",
    "        out = self.conv_net(xb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4460d3c",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ed27c11",
   "metadata": {},
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None,verbose=False):\n",
    "    '''\n",
    "    Apply loss function to a batch of inputs. If no optimizer\n",
    "    is provided, skip the back prop step.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('loss batch ****')\n",
    "        print(\"xb shape:\",xb.shape)\n",
    "        print(\"yb shape:\",yb.shape)\n",
    "        print(\"yb shape:\",yb.squeeze(1).shape)\n",
    "        #print(\"yb\",yb)\n",
    "\n",
    "    # get the batch output from the model given your input batch \n",
    "    # ** This is the model's prediction for the y labels! **\n",
    "    xb_out = model(xb.float())\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"model out pre loss\", xb_out.shape)\n",
    "        #print('xb_out', xb_out)\n",
    "        print(\"xb_out:\",xb_out.shape)\n",
    "        print(\"yb:\",yb.shape)\n",
    "        print(\"yb.long:\",yb.long().shape)\n",
    "    \n",
    "    loss = loss_func(xb_out, yb.float()) # for MSE/regression\n",
    "    # __FOOTNOTE 2__\n",
    "    \n",
    "    if opt is not None: # if opt\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "    \n",
    "def train_step(model, train_dl, loss_func, device, opt):\n",
    "    '''\n",
    "    Execute 1 set of batched training within an epoch\n",
    "    '''\n",
    "    # Set model to Training mode\n",
    "    model.train()\n",
    "    tl = [] # train losses\n",
    "    ns = [] # batch sizes, n\n",
    "    \n",
    "    # loop through train DataLoader\n",
    "    for xb, yb in train_dl:\n",
    "        # put on GPU\n",
    "        xb, yb = xb.to(device),yb.to(device)\n",
    "        \n",
    "        # provide opt so backprop happens\n",
    "        t, n = loss_batch(model, loss_func, xb, yb, opt=opt)\n",
    "        \n",
    "        # collect train loss and batch sizes\n",
    "        tl.append(t)\n",
    "        ns.append(n)\n",
    "    \n",
    "    # average the losses over all batches    \n",
    "    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "def val_step(model, val_dl, loss_func, device):\n",
    "    '''\n",
    "    Execute 1 set of batched validation within an epoch\n",
    "    '''\n",
    "    # Set model to Evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vl = [] # val losses\n",
    "        ns = [] # batch sizes, n\n",
    "        \n",
    "        # loop through validation DataLoader\n",
    "        for xb, yb in val_dl:\n",
    "            # put on GPU\n",
    "            xb, yb = xb.to(device),yb.to(device)\n",
    "\n",
    "            # Do NOT provide opt here, so backprop does not happen\n",
    "            v, n = loss_batch(model, loss_func, xb, yb)\n",
    "\n",
    "            # collect val loss and batch sizes\n",
    "            vl.append(v)\n",
    "            ns.append(n)\n",
    "\n",
    "    # average the losses over all batches\n",
    "    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n",
    "    \n",
    "    return val_loss\n",
    "    \n",
    "def fit(epochs, model, loss_func, opt, train_dl, val_dl,device,patience=1000):\n",
    "    '''\n",
    "    Fit the model params to the training data, eval on unseen data.\n",
    "    Loop for a number of epochs and keep train of train and val losses \n",
    "    along the way\n",
    "    '''\n",
    "    # keep track of losses\n",
    "    train_losses = []    \n",
    "    val_losses = []\n",
    "    \n",
    "    # loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        # take a training step\n",
    "        train_loss = train_step(model,train_dl,loss_func,device,opt)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # take a validation step\n",
    "        val_loss = val_step(model,val_dl,loss_func,device)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"E{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def run_model(train_dl,val_dl,model,device,\n",
    "              lr=0.01, epochs=50, \n",
    "              lossf=None,opt=None\n",
    "             ):\n",
    "    '''\n",
    "    Given train and val DataLoaders and a NN model, fit the mode to the training\n",
    "    data. By default, use MSE loss and an SGD optimizer\n",
    "    '''\n",
    "    # define optimizer\n",
    "    if opt:\n",
    "        optimizer = opt\n",
    "    else: # if no opt provided, just use SGD\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    # define loss function\n",
    "    if lossf:\n",
    "        loss_func = lossf\n",
    "    else: # if no loss function provided, just use MSE\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "    \n",
    "    # run the training loop\n",
    "    train_losses, val_losses = fit(\n",
    "                                epochs, \n",
    "                                model, \n",
    "                                loss_func, \n",
    "                                optimizer, \n",
    "                                train_dl, \n",
    "                                val_dl, \n",
    "                                device)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ef6f3",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19af893e",
   "metadata": {},
   "source": [
    "# get the sequence length from the first seq in the df\n",
    "seq_len = len(train_df['seq'].values[0])\n",
    "\n",
    "# create Linear model object\n",
    "model_lin = DNA_Linear(seq_len)\n",
    "model_lin.to(DEVICE) # put on GPU\n",
    "\n",
    "# run the model with default settings!\n",
    "lin_train_losses, lin_val_losses = run_model(\n",
    "    train_dl, \n",
    "    val_dl, \n",
    "    model_lin,\n",
    "    DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2a649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
